{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "from torch.utils.data import Dataset,DataLoader,TensorDataset\n",
    "from tqdm import tqdm\n",
    "import models.Multi_view_multi_task_ablition_study\n",
    "from torch import nn, optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, average_precision_score,precision_score,f1_score,recall_score\n",
    "from scipy.stats import rankdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()\n",
    "m_gpu=1\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '%d' % m_gpu\n",
    "torch.cuda.set_device(m_gpu)\n",
    "torch.cuda.is_available()\n",
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=3447\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "namedataset=\"Helpdesk\"\n",
    "# namedataset=\"BPI_Challenge_2012_W_Complete\"\n",
    "# namedataset=\"BPI_Challenge_2012_W\"\n",
    "# namedataset=\"BPI_Challenge_2012_A\"\n",
    "# namedataset=\"BPI_Challenge_2012_O\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17549/4047083103.py:60: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_full=df_train.append(df_test).append(df_valid)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/hsc/python_project/paper1/3multi_task_learning_no_task.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B10.1.21.161/home/hsc/python_project/paper1/3multi_task_learning_no_task.ipynb#ch0000002vscode-remote?line=111'>112</a>\u001b[0m res_embsize\u001b[39m=\u001b[39m(num_resource\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m/\u001b[39m\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B10.1.21.161/home/hsc/python_project/paper1/3multi_task_learning_no_task.ipynb#ch0000002vscode-remote?line=112'>113</a>\u001b[0m act_embsize\u001b[39m=\u001b[39m(num_class\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m/\u001b[39m\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2B10.1.21.161/home/hsc/python_project/paper1/3multi_task_learning_no_task.ipynb#ch0000002vscode-remote?line=114'>115</a>\u001b[0m train_data\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39;49mtensor(np\u001b[39m.\u001b[39;49marray(train_data),dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfloat32)\u001b[39m.\u001b[39;49mcuda()\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B10.1.21.161/home/hsc/python_project/paper1/3multi_task_learning_no_task.ipynb#ch0000002vscode-remote?line=115'>116</a>\u001b[0m train_label\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mtensor(np\u001b[39m.\u001b[39marray(train_label),dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32)\u001b[39m.\u001b[39mcuda()\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B10.1.21.161/home/hsc/python_project/paper1/3multi_task_learning_no_task.ipynb#ch0000002vscode-remote?line=116'>117</a>\u001b[0m test_data\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mtensor(np\u001b[39m.\u001b[39marray(test_data),dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32)\u001b[39m.\u001b[39mcuda()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "results=[]\n",
    "for step in range(5):\n",
    "    fold=\"fold\"+str(step)\n",
    "    train_file=\"processed_datasets/\"+namedataset+\"/fold\"+str(step)+\"/train_\"+namedataset+\".csv\"\n",
    "    val_file=\"processed_datasets/\"+namedataset+\"/fold\"+str(step)+\"/val_\"+namedataset+\".csv\"\n",
    "    test_file=\"processed_datasets/\"+namedataset+\"/fold\"+str(step)+\"/test_\"+namedataset+\".csv\"\n",
    "    df_test=pd.read_csv(test_file)\n",
    "    alpha=0.3\n",
    "    fold=\"fold\"+str(step)\n",
    "    pattern_act_dict=pickle.load(open(\"datasets_results/\"+namedataset+\"/\"+fold+\"/pattern_act_dict_\"+str(alpha)+\".pkl\",\"rb\"))\n",
    "    pattern_act_time_dict=pickle.load(open(\"datasets_results/\"+namedataset+\"/\"+fold+\"/pattern_act_time_dict_\"+str(alpha)+\".pkl\",\"rb\"))\n",
    "    pattern_act_res_dict=pickle.load(open(\"datasets_results/\"+namedataset+\"/\"+fold+\"/pattern_act_res_dict_\"+str(alpha)+\".pkl\",\"rb\"))\n",
    "    res_cluster_label=pickle.load(open(\"datasets_results/\"+namedataset+\"/\"+fold+\"/res_cluster_label_\"+str(alpha)+\".pkl\",\"rb\"))\n",
    "    res_cluster_num=max(res_cluster_label)+1\n",
    "\n",
    "    pattern_act_sup_list=[]\n",
    "    for key in pattern_act_dict:\n",
    "        # print(len(pattern_act_dict[key]))\n",
    "        pattern_act_sup_list.append(len(pattern_act_dict[key]))\n",
    "\n",
    "    pattern_act_res_sup_list=[]\n",
    "    for key in pattern_act_res_dict:\n",
    "        # print(len(pattern_act_dict[key]))\n",
    "        pattern_act_res_sup_list.append(len(pattern_act_res_dict[key]))\n",
    "\n",
    "    pattern_act_time_sup_list=[]\n",
    "    for key in pattern_act_time_dict:\n",
    "        # print(len(pattern_act_dict[key]))\n",
    "        pattern_act_time_sup_list.append(len(pattern_act_time_dict[key]))\n",
    "\n",
    "    try:\n",
    "        df_test.columns = [\"CaseID\", \"Activity\", \"Resource\", \"Timestamp\",\"amount\"]\n",
    "    except:\n",
    "        df_test.columns = [\"CaseID\", \"Activity\", \"Resource\", \"Timestamp\"]\n",
    "\n",
    "    df_train=pd.read_csv(train_file)\n",
    "    try:\n",
    "        df_train.columns = [\"CaseID\", \"Activity\", \"Resource\", \"Timestamp\",\"amount\"]\n",
    "    except:\n",
    "        df_train.columns = [\"CaseID\", \"Activity\", \"Resource\", \"Timestamp\"]\n",
    "\n",
    "    df_valid=pd.read_csv(val_file)\n",
    "    try:\n",
    "        df_valid.columns = [\"CaseID\", \"Activity\", \"Resource\", \"Timestamp\",\"amount\"]\n",
    "    except:\n",
    "        df_valid.columns = [\"CaseID\", \"Activity\", \"Resource\", \"Timestamp\"]\n",
    "\n",
    "    df_train.fillna(1, inplace=True)\n",
    "    df_test.fillna(1,inplace=True)\n",
    "    df_valid.fillna(1,inplace=True)\n",
    "\n",
    "    cont_trace = df_train['CaseID'].value_counts(dropna=False)\n",
    "    cont_test_trace = df_test['CaseID'].value_counts(dropna=False)\n",
    "    cont_val_trace = df_valid['CaseID'].value_counts(dropna=False)\n",
    "\n",
    "    max_trace = max(cont_trace)\n",
    "    max_test_trace = max(cont_test_trace)\n",
    "    max_val_trace = max(cont_val_trace)\n",
    "    real_max=max(max_trace,max_test_trace,max_val_trace)\n",
    "    df_full=df_train.append(df_test).append(df_valid)\n",
    "    mean_trace=int(round(np.mean(df_full['CaseID'].value_counts(dropna=False))))\n",
    "    \n",
    "    test_listOfResource=df_test[\"Resource\"].unique().tolist()\n",
    "    train_listOfResource=df_train[\"Resource\"].unique().tolist()\n",
    "    valid_listOfResource=df_valid[\"Resource\"].unique().tolist()\n",
    "\n",
    "    test_resourceset=set(test_listOfResource)\n",
    "    train_resourceset=set(train_listOfResource)\n",
    "    valid_resourceset=set(valid_listOfResource)\n",
    "    all_resource=list(valid_resourceset.union(test_resourceset.union(train_resourceset)))\n",
    "    all_resource.sort()\n",
    "    listOfresourcesInt = list(range(0, len(all_resource)))\n",
    "    resourceMapping=dict(zip(all_resource,listOfresourcesInt))\n",
    "\n",
    "    test_listOfEvents=df_test[\"Activity\"].unique().tolist()\n",
    "    train_listOfEvents=df_train[\"Activity\"].unique().tolist()\n",
    "    valid_listOfEvents=df_valid[\"Activity\"].unique().tolist()\n",
    "    test_eventset=set(test_listOfEvents)\n",
    "    train_eventset=set(train_listOfEvents)\n",
    "    valid_eventset=set(valid_listOfEvents)\n",
    "    all_events=list(valid_eventset.union(test_eventset.union(train_eventset)))\n",
    "    all_events.sort()\n",
    "    listOfeventsInt = list(range(0, len(all_events)))\n",
    "    mapping = dict(zip(all_events, listOfeventsInt))\n",
    "\n",
    "    res_cluster_label=pickle.load(open(\"datasets_results/\"+namedataset+\"/\"+fold+\"/res_cluster_label_\"+str(alpha)+\".pkl\",\"rb\"))\n",
    "    res_cluster_num=max(res_cluster_label)+1\n",
    "    remain_list=pickle.load(open(\"datasets_results/\"+namedataset+\"/\"+fold+\"/time_normlize.pkl\",\"rb\"))\n",
    "    real_remain=max(remain_list)\n",
    "        \n",
    "    train_data=pickle.load(open(\"datasets_results/\"+namedataset+\"/\"+fold+\"/\"+\"trace_length_\"+str(mean_trace)+\"/\"+\"train_data_\"+str(alpha)+\".pkl\",\"rb\"))\n",
    "    train_label=pickle.load(open(\"datasets_results/\"+namedataset+\"/\"+fold+\"/\"+\"trace_length_\"+str(mean_trace)+\"/\"+\"train_label_\"+str(alpha)+\".pkl\",\"rb\"))\n",
    "    train_act_pattern_list=pickle.load(open(\"datasets_results/\"+namedataset+\"/\"+fold+\"/\"+\"trace_length_\"+str(mean_trace)+\"/\"+\"train_act_pattern_list_\"+str(alpha)+\".pkl\",\"rb\"))\n",
    "    train_act_res_pattern_list=pickle.load(open(\"datasets_results/\"+namedataset+\"/\"+fold+\"/\"+\"trace_length_\"+str(mean_trace)+\"/\"+\"train_act_res_pattern_list_\"+str(alpha)+\".pkl\",\"rb\"))\n",
    "    train_act_time_pattern_list=pickle.load(open(\"datasets_results/\"+namedataset+\"/\"+fold+\"/\"+\"trace_length_\"+str(mean_trace)+\"/\"+\"train_act_time_pattern_list_\"+str(alpha)+\".pkl\",\"rb\"))\n",
    "\n",
    "    test_data=pickle.load(open(\"datasets_results/\"+namedataset+\"/\"+fold+\"/\"+\"trace_length_\"+str(mean_trace)+\"/\"+\"test_data_\"+str(alpha)+\".pkl\",\"rb\"))\n",
    "    test_label=pickle.load(open(\"datasets_results/\"+namedataset+\"/\"+fold+\"/\"+\"trace_length_\"+str(mean_trace)+\"/\"+\"test_label_\"+str(alpha)+\".pkl\",\"rb\"))\n",
    "    test_act_pattern_list=pickle.load(open(\"datasets_results/\"+namedataset+\"/\"+fold+\"/\"+\"trace_length_\"+str(mean_trace)+\"/\"+\"test_act_pattern_list_\"+str(alpha)+\".pkl\",\"rb\"))\n",
    "    test_act_res_pattern_list=pickle.load(open(\"datasets_results/\"+namedataset+\"/\"+fold+\"/\"+\"trace_length_\"+str(mean_trace)+\"/\"+\"test_act_res_pattern_list_\"+str(alpha)+\".pkl\",\"rb\"))\n",
    "    test_act_time_pattern_list=pickle.load(open(\"datasets_results/\"+namedataset+\"/\"+fold+\"/\"+\"trace_length_\"+str(mean_trace)+\"/\"+\"test_act_time_pattern_list_\"+str(alpha)+\".pkl\",\"rb\"))\n",
    "\n",
    "    valid_data=pickle.load(open(\"datasets_results/\"+namedataset+\"/\"+fold+\"/\"+\"trace_length_\"+str(mean_trace)+\"/\"+\"valid_data_\"+str(alpha)+\".pkl\",\"rb\"))\n",
    "    valid_label=pickle.load(open(\"datasets_results/\"+namedataset+\"/\"+fold+\"/\"+\"trace_length_\"+str(mean_trace)+\"/\"+\"valid_label_\"+str(alpha)+\".pkl\",\"rb\"))\n",
    "    valid_act_pattern_list=pickle.load(open(\"datasets_results/\"+namedataset+\"/\"+fold+\"/\"+\"trace_length_\"+str(mean_trace)+\"/\"+\"valid_act_pattern_list_\"+str(alpha)+\".pkl\",\"rb\"))\n",
    "    valid_act_res_pattern_list=pickle.load(open(\"datasets_results/\"+namedataset+\"/\"+fold+\"/\"+\"trace_length_\"+str(mean_trace)+\"/\"+\"valid_act_res_pattern_list_\"+str(alpha)+\".pkl\",\"rb\"))\n",
    "    valid_act_time_pattern_list=pickle.load(open(\"datasets_results/\"+namedataset+\"/\"+fold+\"/\"+\"trace_length_\"+str(mean_trace)+\"/\"+\"valid_act_time_pattern_list_\"+str(alpha)+\".pkl\",\"rb\"))\n",
    "\n",
    "    num_resource=len(all_resource)+1\n",
    "    num_class=len(all_events)+1\n",
    "\n",
    "    res_embsize=(num_resource+1)//2\n",
    "    act_embsize=(num_class+1)//2\n",
    "\n",
    "    train_data=torch.tensor(np.array(train_data),dtype=torch.float32).cuda()\n",
    "    train_label=torch.tensor(np.array(train_label),dtype=torch.float32).cuda()\n",
    "    test_data=torch.tensor(np.array(test_data),dtype=torch.float32).cuda()\n",
    "    test_label=torch.tensor(np.array(test_label),dtype=torch.float32).cuda()\n",
    "    valid_data=torch.tensor(np.array(valid_data),dtype=torch.float32).cuda()\n",
    "    valid_label=torch.tensor(np.array(valid_label),dtype=torch.float32).cuda()\n",
    "\n",
    "    \n",
    "    train_act_pattern=torch.tensor(np.array(train_act_pattern_list),dtype=torch.long).cuda()\n",
    "    train_act_res_pattern=torch.tensor(np.array(train_act_res_pattern_list),dtype=torch.long).cuda()\n",
    "    train_act_time_pattern=torch.tensor(np.array(train_act_time_pattern_list),dtype=torch.long).cuda()\n",
    "    valid_act_pattern=torch.tensor(np.array(valid_act_pattern_list),dtype=torch.long).cuda()\n",
    "    valid_act_res_pattern=torch.tensor(np.array(valid_act_res_pattern_list),dtype=torch.long).cuda()\n",
    "    valid_act_time_pattern=torch.tensor(np.array(valid_act_time_pattern_list),dtype=torch.long).cuda()\n",
    "    test_act_pattern=torch.tensor(np.array(test_act_pattern_list),dtype=torch.long).cuda()\n",
    "    test_act_res_pattern=torch.tensor(np.array(test_act_res_pattern_list),dtype=torch.long).cuda()\n",
    "    test_act_time_pattern=torch.tensor(np.array(test_act_time_pattern_list),dtype=torch.long).cuda()\n",
    "\n",
    "    batch_size=128\n",
    "    train_dataset=TensorDataset(train_data,train_label[:,0],train_label[:,1],train_label[:,5],train_act_pattern,train_act_res_pattern,train_act_time_pattern)\n",
    "    train_dataloader=DataLoader(dataset=train_dataset,\n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=True\n",
    "                                )\n",
    "    valid_dataset=TensorDataset(valid_data,valid_label[:,0],valid_label[:,1],valid_label[:,5],valid_act_pattern,valid_act_res_pattern,valid_act_time_pattern)\n",
    "    valid_dataloader=DataLoader(dataset=valid_dataset,\n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=True\n",
    "                                )\n",
    "    \n",
    "    test_dataset=TensorDataset(test_data,test_label[:,0],test_label[:,1],test_label[:,5],test_act_pattern,test_act_res_pattern,test_act_time_pattern)\n",
    "    test_dataloader=DataLoader(dataset=test_dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=True\n",
    "                            )\n",
    "\n",
    "    hidden_dim=20\n",
    "    embsize_list=[act_embsize,res_embsize]\n",
    "    catnum_list=[num_class,num_resource]\n",
    "    num_size=2+7+24\n",
    "    num_timestep=train_data.shape[1]\n",
    "    embsize_list=[act_embsize,res_embsize]\n",
    "    num_list=[num_class,num_resource]\n",
    "    num_layers=3\n",
    "    kernel_size=2\n",
    "    dropout=0.3\n",
    "    pattern_embsize_list=[act_embsize,32,32]\n",
    "    pattern_num_list=[num_class,res_cluster_num,10]\n",
    "                        \n",
    "    model=models.Multi_view_multi_task_ablition_study.Multi_view_multi_task_no_task_relevance(embsize_list,num_list,hidden_dim,kernel_size,dropout,num_layers,pattern_embsize_list,pattern_num_list).cuda()\n",
    "    \n",
    "    class MTLLoss(nn.Module):\n",
    "        def __init__(self, task_nums):\n",
    "            super(MTLLoss, self).__init__()\n",
    "            x = torch.zeros([task_nums], dtype=torch.float32)\n",
    "            self.log_var2s = nn.Parameter(x)\n",
    "        def forward(self,loss_list):\n",
    "            loss = 0\n",
    "            for i in range(len(self.log_var2s)):\n",
    "                # mse = (logit_list[i] - label_list[i]) ** 2\n",
    "                pre = torch.exp(-self.log_var2s[i])\n",
    "                loss += torch.sum(pre * loss_list[i] + self.log_var2s[i], dim=-1)\n",
    "            return torch.mean(loss)\n",
    "    lr=1e-3\n",
    "    my_optim=\"Adam\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    mae=nn.L1Loss()\n",
    "    mtlloss=MTLLoss(3).cuda()\n",
    "    # 定义GradNorm参数\n",
    "    task_numbers = 3 # 任务数\n",
    "    mu = 1e-2 # GradNorm参数\n",
    "    optimizer = getattr(optim, my_optim)([{\"params\":model.parameters()},{\"params\":mtlloss.parameters()}], lr=lr)\n",
    "    ExpLR = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.98)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    bestloss=1e6\n",
    "\n",
    "    def test(ttest_dataLoader):\n",
    "        model.eval()\n",
    "        tloss=0\n",
    "        \n",
    "        act_pred=np.array([])\n",
    "        act_truth=np.array([])\n",
    "\n",
    "        res_pred=np.array([])\n",
    "        res_truth=np.array([])\n",
    "        \n",
    "        time_pred=np.array([])\n",
    "        time_truth=np.array([])\n",
    "\n",
    "        test_data=np.array([])\n",
    "        test_caseid=np.array([])\n",
    "\n",
    "        test_length=np.array([])\n",
    "\n",
    "        for j,(data,ylabels,res_labels,time_labels,act_pattern,act_res_pattern,act_time_pattern) in enumerate(ttest_dataLoader):\n",
    "            optimizer.zero_grad()\n",
    "            actoutput,resoutput,remain = model(data,act_pattern,act_res_pattern,act_time_pattern)\n",
    "            # actoutput,resoutput,remain = model(data)\n",
    "\n",
    "            # actoutput,timeoutput,resoutput,remain_output = model(data,trans_prob)\n",
    "\n",
    "            # loss1 = criterion(actoutput.squeeze(dim=2), label.long())\n",
    "            loss1 = criterion(actoutput, ylabels.long().reshape(data.size(0)))\n",
    "            loss2 = criterion(resoutput,res_labels.long().reshape(data.size(0)))\n",
    "            loss3 = mae(remain,time_labels.reshape(data.size(0),1))\n",
    "            # ExpLR.step()\n",
    "            tloss+=(loss3.item()*data.shape[0])\n",
    "            # (0.5*loss1+0.5*loss2).backward()\n",
    "            # optimizer.step()\n",
    "            softmaxoutput=nn.functional.softmax(actoutput,dim=1)\n",
    "            predicted=torch.argmax(softmaxoutput,dim=1)\n",
    "            groundTruth=ylabels\n",
    "            res_predicted=torch.argmax(resoutput,dim=1)\n",
    "            # res_truth=label[1]\n",
    "\n",
    "            act_pred=np.append(act_pred,predicted.cpu().numpy())\n",
    "            act_truth=np.append(act_truth,groundTruth.cpu().numpy())\n",
    "\n",
    "            res_pred=np.append(res_pred,res_predicted.cpu().numpy())\n",
    "            res_truth=np.append(res_truth,res_labels.cpu().numpy())\n",
    "\n",
    "            time_pred=np.append(time_pred,remain.detach().cpu().numpy())\n",
    "            time_truth=np.append(time_truth,time_labels.cpu().numpy())\n",
    "            \n",
    "            test_data=np.append(test_data,data[:,:,0].cpu().numpy()) \n",
    "\n",
    "            # test_length=np.append(test_length,length_labels.cpu().numpy())\n",
    "\n",
    "        return act_pred,act_truth,res_pred,res_truth,time_pred,time_truth,_,tloss\n",
    "\n",
    "    def valid(ttest_dataLoader):\n",
    "        model.eval()\n",
    "        tloss=0\n",
    "        \n",
    "        act_pred=np.array([])\n",
    "        act_truth=np.array([])\n",
    "\n",
    "        res_pred=np.array([])\n",
    "        res_truth=np.array([])\n",
    "        \n",
    "        time_pred=np.array([])\n",
    "        time_truth=np.array([])\n",
    "\n",
    "        test_data=np.array([])\n",
    "        test_caseid=np.array([])\n",
    "\n",
    "        test_length=np.array([])\n",
    "\n",
    "        for j,(data,ylabels,res_labels,time_labels,act_pattern,act_res_pattern,act_time_pattern) in enumerate(ttest_dataLoader):\n",
    "            optimizer.zero_grad()\n",
    "            actoutput,resoutput,remain = model(data,act_pattern,act_res_pattern,act_time_pattern)\n",
    "            # actoutput,resoutput,remain = model(data)\n",
    "\n",
    "            # actoutput,timeoutput,resoutput,remain_output = model(data,trans_prob)\n",
    "\n",
    "            # loss1 = criterion(actoutput.squeeze(dim=2), label.long())\n",
    "            loss1 = criterion(actoutput, ylabels.long().reshape(data.size(0)))\n",
    "            loss2 = criterion(resoutput,res_labels.long().reshape(data.size(0)))\n",
    "            loss3 = mae(remain,time_labels.reshape(data.size(0),1))\n",
    "            # ExpLR.step()\n",
    "            tloss+=(loss3.item()*data.shape[0])\n",
    "            # (0.5*loss1+0.5*loss2).backward()\n",
    "            # optimizer.step()\n",
    "            softmaxoutput=nn.functional.softmax(actoutput,dim=1)\n",
    "            predicted=torch.argmax(softmaxoutput,dim=1)\n",
    "            groundTruth=ylabels\n",
    "            res_predicted=torch.argmax(resoutput,dim=1)\n",
    "            # res_truth=label[1]\n",
    "\n",
    "            act_pred=np.append(act_pred,predicted.cpu().numpy())\n",
    "            act_truth=np.append(act_truth,groundTruth.cpu().numpy())\n",
    "\n",
    "            res_pred=np.append(res_pred,res_predicted.cpu().numpy())\n",
    "            res_truth=np.append(res_truth,res_labels.cpu().numpy())\n",
    "\n",
    "            time_pred=np.append(time_pred,remain.detach().cpu().numpy())\n",
    "            time_truth=np.append(time_truth,time_labels.cpu().numpy())\n",
    "            \n",
    "            test_data=np.append(test_data,data[:,:,0].cpu().numpy()) \n",
    "\n",
    "            # test_length=np.append(test_length,length_labels.cpu().numpy())\n",
    "\n",
    "        return act_pred,act_truth,res_pred,res_truth,time_pred,time_truth,_,tloss\n",
    "\n",
    "    def cal_accuracy_precision_f1score(y_true,y_pred):\n",
    "        recall=recall_score(y_true,y_pred,average=\"weighted\",zero_division=1)\n",
    "        precision=precision_score(y_true,y_pred,average=\"weighted\",zero_division=1)\n",
    "        f1score=f1_score(y_true,y_pred,average=\"weighted\",zero_division=1)\n",
    "        return recall,precision,f1score\n",
    "    \n",
    "    def cal_mse(y_true,y_pred):\n",
    "        y_pred=y_pred*(real_remain/(24*60*60))\n",
    "        y_true=y_true*(real_remain/(24*60*60))\n",
    "        return np.mean((y_true - y_pred) ** 2)\n",
    "    def cal_mae(y_true,y_pred):\n",
    "        y_pred=y_pred*(real_remain/(24*60*60))\n",
    "        y_true=y_true*(real_remain/(24*60*60))\n",
    "        return np.mean(np.abs(y_pred-y_true))\n",
    "\n",
    "    def train(epoch):\n",
    "        valid_act_acc_list=[]\n",
    "        valid_act_pre_list=[]\n",
    "        valid_act_fscore_list=[]\n",
    "        valid_res_acc_list=[]\n",
    "        valid_res_pre_list=[]\n",
    "        valid_res_fscore_list=[]\n",
    "        valid_time_mae_list=[]\n",
    "        valid_time_mse_list=[]\n",
    "        test_act_acc_list=[]\n",
    "        test_act_pre_list=[]\n",
    "        test_act_fscore_list=[]\n",
    "        test_res_acc_list=[]\n",
    "        test_res_pre_list=[]\n",
    "        test_res_fscore_list=[]\n",
    "        test_time_mae_list=[]\n",
    "        test_time_mse_list=[]\n",
    "\n",
    "        for m in tqdm(range(epoch)):\n",
    "            print(\"epoch  \",m)\n",
    "            model.train()\n",
    "            t_loss=[]\n",
    "            trainpred=np.array([])\n",
    "            traintruth=np.array([])\n",
    "            train_respred=np.array([])\n",
    "            train_restruth=np.array([])\n",
    "            # print(gradnormloss.w)\n",
    "            for j,(data,label,reslabel,timelabel,act_pattern,act_res_pattern,act_time_pattern) in enumerate(train_dataloader):\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                actoutput,resoutput,timeoutput = model(data,act_pattern,act_res_pattern,act_time_pattern)\n",
    "                # actoutput,resoutput,timeoutput = model(data)\n",
    "\n",
    "                # loss1 = criterion(actoutput.squeeze(dim=2), label.long())\n",
    "                loss1 = criterion(actoutput, label.long().reshape(data.size(0)))\n",
    "                loss3 = mae(timeoutput,timelabel.reshape(data.size(0),1))\n",
    "                loss2 = criterion(resoutput, reslabel.long().reshape(data.size(0)))\n",
    "        \n",
    "                # paraments=model.get_last_layer_shared()\n",
    "                # gradnormloss(torch.cat((loss1,loss2),dim=1),paraments,optimizer,None)\n",
    "                # (loss1*0.2+loss2*1.8).backward()\n",
    "                # (loss1*0.5+loss3*0.5).backward()\n",
    "                # (0.7*loss1+0.2*loss2+0.1*loss3).backward()\n",
    "                # loss1.backward()\n",
    "                loss=mtlloss([loss1,loss2,loss3])\n",
    "                loss.backward()\n",
    "                # loss3.backward()\n",
    "                optimizer.step()\n",
    "                t_loss.append([loss1.item(),loss2.item(),loss3.item()])\n",
    "                softmaxoutput=nn.functional.softmax(actoutput,dim=1)\n",
    "                predicted=torch.argmax(softmaxoutput,dim=1)\n",
    "                groundTruth=label\n",
    "                trainpred=np.append(trainpred,predicted.cpu().numpy())\n",
    "                traintruth=np.append(traintruth,groundTruth.cpu().numpy())\n",
    "\n",
    "                res_predicted=torch.argmax(resoutput,dim=1)\n",
    "                res_groundTruth=reslabel\n",
    "                train_respred=np.append(train_respred,res_predicted.cpu().numpy())\n",
    "                train_restruth=np.append(train_restruth,res_groundTruth.cpu().numpy()) \n",
    "            scheduler.step()\n",
    "            model.eval()\n",
    "\n",
    "            act_pred,act_truth,res_pred,res_truth,time_pred,time_truth,test_length,tloss=test(test_dataloader)\n",
    "            val_act_pred,val_act_truth,val_res_pred,val_res_truth,val_time_pred,val_time_truth,test_length,tloss=valid(valid_dataloader)\n",
    "\n",
    "            test_act_acc,test_act_pre,test_act_fscore=cal_accuracy_precision_f1score(act_truth,act_pred)\n",
    "            test_res_acc,test_res_pre,test_res_fscore=cal_accuracy_precision_f1score(res_truth,res_pred)\n",
    "            time_mae=cal_mae(time_truth,time_pred)\n",
    "            time_mse=cal_mse(time_truth,time_pred)\n",
    "\n",
    "            valid_act_acc,valid_act_pre,valid_act_fscore=cal_accuracy_precision_f1score(val_act_truth,val_act_pred)\n",
    "            valid_res_acc,valid_res_pre,valid_res_fscore=cal_accuracy_precision_f1score(val_res_truth,val_res_pred)\n",
    "            valid_time_mae=cal_mae(val_time_truth,val_time_pred)\n",
    "            valid_time_mse=cal_mse(val_time_truth,val_time_pred)\n",
    "\n",
    "            valid_act_acc_list.append(valid_act_acc)\n",
    "            valid_act_pre_list.append(valid_act_pre)\n",
    "            valid_act_fscore_list.append(valid_act_fscore)\n",
    "            valid_res_acc_list.append(valid_res_acc)\n",
    "            valid_res_pre_list.append(valid_res_pre)\n",
    "            valid_res_fscore_list.append(valid_res_fscore)\n",
    "            valid_time_mae_list.append(valid_time_mae)\n",
    "            valid_time_mse_list.append(valid_time_mse)\n",
    "\n",
    "\n",
    "            test_act_acc_list.append(test_act_acc)\n",
    "            test_act_pre_list.append(test_act_pre)\n",
    "            test_act_fscore_list.append(test_act_fscore)\n",
    "            test_res_acc_list.append(test_res_acc)\n",
    "            test_res_pre_list.append(test_res_pre)\n",
    "            test_res_fscore_list.append(test_res_fscore)\n",
    "            test_time_mae_list.append(time_mae)\n",
    "            test_time_mse_list.append(time_mse)\n",
    "        \n",
    "        def rank(score,mode=1):\n",
    "            score=np.array(score)\n",
    "            ranks=rankdata(score)\n",
    "            if mode==1:\n",
    "                return ranks\n",
    "            else:\n",
    "                ranks=len(score)+1-ranks\n",
    "                return ranks\n",
    "\n",
    "        act_acc_rank=rank(valid_act_acc_list)\n",
    "        act_pre_rank=rank(valid_act_pre_list)\n",
    "        act_fscore_rank=rank(valid_act_fscore_list)\n",
    "        res_acc_rank=rank(valid_res_acc_list)\n",
    "        res_pre_rank=rank(valid_res_pre_list)\n",
    "        res_fscore_rank=rank(valid_res_fscore_list)\n",
    "        time_mae_rank=rank(valid_time_mae_list)\n",
    "        time_mse_rank=rank(valid_time_mse_list)\n",
    "        # ultra_rank=act_acc_rank+act_pre_rank+act_fscore_rank+res_acc_rank+res_pre_rank+res_fscore_rank+time_mae_rank+time_mse_rank\n",
    "        ultra_rank=act_acc_rank+res_acc_rank+0.4*time_mae_rank+0.4*time_mse_rank\n",
    "        \n",
    "        minindex=np.argmin(ultra_rank)\n",
    "        tempresult=[step,test_act_acc_list[minindex],test_act_pre_list[minindex],\n",
    "        test_act_fscore_list[minindex],test_res_acc_list[minindex],test_res_pre_list[minindex],test_res_fscore_list[minindex],\n",
    "        test_time_mae_list[minindex],test_time_mse_list[minindex]]\n",
    "        results.append(tempresult)\n",
    "    train(30)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"results/\"+namedataset):\n",
    "    os.mkdir(\"results/\"+namedataset)\n",
    "f=open(\"results/\"+namedataset+\"/no_task_results.csv\",\"w\")\n",
    "csv_writer = csv.writer(f)\n",
    "csv_writer.writerow([\"fold\",\"act_acc\",\"act_pre\",\"act_fscore\",\"res_acc\",\"res_pre\",\"res_fscore\",\"time_mae\",\"time_mse\"])\n",
    "for i in range(5):\n",
    "    csv_writer.writerow(results[i])\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2e67272651055d1832734db27dbe5c78ae7e6195044c32c87375667fa374755b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
