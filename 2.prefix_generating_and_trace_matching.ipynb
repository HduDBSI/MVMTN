{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "from collections import Counter\n",
    "import copy\n",
    "import pickle\n",
    "import os \n",
    "import csv\n",
    "import re\n",
    "import Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# namedataset=\"Helpdesk\"\n",
    "# namedataset=\"BPI_Challenge_2012_W_Complete\"\n",
    "namedataset=\"BPI_Challenge_2012_W\"\n",
    "# namedataset=\"BPI_Challenge_2012_A\"\n",
    "# namedataset=\"BPI_Challenge_2012_O\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26426/1779962214.py:59: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_full=df_train.append(df_test).append(df_valid)\n",
      "/tmp/ipykernel_26426/1779962214.py:59: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_full=df_train.append(df_test).append(df_valid)\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device: 'datasets_results/BPI_Challenge_2012_W/fold1/trace_length_18'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m/home/hsc/python_project/paper1/2.prefix_generating_and_trace_matching.ipynb Cell 3'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B10.1.21.161/home/hsc/python_project/paper1/2.prefix_generating_and_trace_matching.ipynb#ch0000002vscode-remote?line=314'>315</a>\u001b[0m     data[\u001b[39m5\u001b[39m]\u001b[39m=\u001b[39m(data[\u001b[39m5\u001b[39m])\u001b[39m/\u001b[39m(real_max_remain)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B10.1.21.161/home/hsc/python_project/paper1/2.prefix_generating_and_trace_matching.ipynb#ch0000002vscode-remote?line=316'>317</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(\u001b[39m\"\u001b[39m\u001b[39mdatasets_results/\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39mnamedataset\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39mfold\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtrace_length_\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(mean_trace)):\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2B10.1.21.161/home/hsc/python_project/paper1/2.prefix_generating_and_trace_matching.ipynb#ch0000002vscode-remote?line=317'>318</a>\u001b[0m     os\u001b[39m.\u001b[39;49mmkdir(\u001b[39m\"\u001b[39;49m\u001b[39mdatasets_results/\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m+\u001b[39;49mnamedataset\u001b[39m+\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m/\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m+\u001b[39;49mfold\u001b[39m+\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m/\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m+\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtrace_length_\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m+\u001b[39;49m\u001b[39mstr\u001b[39;49m(mean_trace))\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B10.1.21.161/home/hsc/python_project/paper1/2.prefix_generating_and_trace_matching.ipynb#ch0000002vscode-remote?line=319'>320</a>\u001b[0m pickle\u001b[39m.\u001b[39mdump(train_data,\u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mdatasets_results/\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39mnamedataset\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39mfold\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtrace_length_\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(mean_trace)\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtrain_data_\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(alpha)\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.pkl\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B10.1.21.161/home/hsc/python_project/paper1/2.prefix_generating_and_trace_matching.ipynb#ch0000002vscode-remote?line=320'>321</a>\u001b[0m pickle\u001b[39m.\u001b[39mdump(train_label,\u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mdatasets_results/\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39mnamedataset\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39mfold\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtrace_length_\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(mean_trace)\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtrain_label_\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(alpha)\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.pkl\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device: 'datasets_results/BPI_Challenge_2012_W/fold1/trace_length_18'"
     ]
    }
   ],
   "source": [
    "for step in range(5):\n",
    "    # train_val_file=\"processed_datasets/\"+namedataset+\"/fold\"+str(step)+\"/train_val_\"+namedataset+\".csv\"\n",
    "    train_file=\"processed_datasets/\"+namedataset+\"/fold\"+str(step)+\"/train_\"+namedataset+\".csv\"\n",
    "    val_file=\"processed_datasets/\"+namedataset+\"/fold\"+str(step)+\"/val_\"+namedataset+\".csv\"\n",
    "    test_file=\"processed_datasets/\"+namedataset+\"/fold\"+str(step)+\"/test_\"+namedataset+\".csv\"\n",
    "    df_test=pd.read_csv(test_file)\n",
    "    alpha=0.3\n",
    "    fold=\"fold\"+str(step)\n",
    "    pattern_act_dict=pickle.load(open(\"datasets_results/\"+namedataset+\"/\"+fold+\"/pattern_act_dict_\"+str(alpha)+\".pkl\",\"rb\"))\n",
    "    pattern_act_time_dict=pickle.load(open(\"datasets_results/\"+namedataset+\"/\"+fold+\"/pattern_act_time_dict_\"+str(alpha)+\".pkl\",\"rb\"))\n",
    "    pattern_act_res_dict=pickle.load(open(\"datasets_results/\"+namedataset+\"/\"+fold+\"/pattern_act_res_dict_\"+str(alpha)+\".pkl\",\"rb\"))\n",
    "    res_cluster_label=pickle.load(open(\"datasets_results/\"+namedataset+\"/\"+fold+\"/res_cluster_label_\"+str(alpha)+\".pkl\",\"rb\"))\n",
    "    res_cluster_num=max(res_cluster_label)+1\n",
    "\n",
    "    pattern_act_sup_list=[]\n",
    "    for key in pattern_act_dict:\n",
    "        # print(len(pattern_act_dict[key]))\n",
    "        pattern_act_sup_list.append(len(pattern_act_dict[key]))\n",
    "\n",
    "    pattern_act_res_sup_list=[]\n",
    "    for key in pattern_act_res_dict:\n",
    "        # print(len(pattern_act_dict[key]))\n",
    "        pattern_act_res_sup_list.append(len(pattern_act_res_dict[key]))\n",
    "\n",
    "    pattern_act_time_sup_list=[]\n",
    "    for key in pattern_act_time_dict:\n",
    "        # print(len(pattern_act_dict[key]))\n",
    "        pattern_act_time_sup_list.append(len(pattern_act_time_dict[key]))\n",
    "\n",
    "    try:\n",
    "        df_test.columns = [\"CaseID\", \"Activity\", \"Resource\", \"Timestamp\",\"amount\"]\n",
    "    except:\n",
    "        df_test.columns = [\"CaseID\", \"Activity\", \"Resource\", \"Timestamp\"]\n",
    "\n",
    "    df_train=pd.read_csv(train_file)\n",
    "    try:\n",
    "        df_train.columns = [\"CaseID\", \"Activity\", \"Resource\", \"Timestamp\",\"amount\"]\n",
    "    except:\n",
    "        df_train.columns = [\"CaseID\", \"Activity\", \"Resource\", \"Timestamp\"]\n",
    "\n",
    "    df_valid=pd.read_csv(val_file)\n",
    "    try:\n",
    "        df_valid.columns = [\"CaseID\", \"Activity\", \"Resource\", \"Timestamp\",\"amount\"]\n",
    "    except:\n",
    "        df_valid.columns = [\"CaseID\", \"Activity\", \"Resource\", \"Timestamp\"]\n",
    "\n",
    "    df_train.fillna(1, inplace=True)\n",
    "    df_test.fillna(1,inplace=True)\n",
    "    df_valid.fillna(1,inplace=True)\n",
    "\n",
    "    cont_trace = df_train['CaseID'].value_counts(dropna=False)\n",
    "    cont_test_trace = df_test['CaseID'].value_counts(dropna=False)\n",
    "    cont_val_trace = df_valid['CaseID'].value_counts(dropna=False)\n",
    "\n",
    "    max_trace = max(cont_trace)\n",
    "    max_test_trace = max(cont_test_trace)\n",
    "    max_val_trace = max(cont_val_trace)\n",
    "    real_max=max(max_trace,max_test_trace,max_val_trace)\n",
    "    df_full=df_train.append(df_test).append(df_valid)\n",
    "    mean_trace=int(round(np.mean(df_full['CaseID'].value_counts(dropna=False))))\n",
    "    \n",
    "    test_listOfResource=df_test[\"Resource\"].unique().tolist()\n",
    "    train_listOfResource=df_train[\"Resource\"].unique().tolist()\n",
    "    valid_listOfResource=df_valid[\"Resource\"].unique().tolist()\n",
    "\n",
    "    test_resourceset=set(test_listOfResource)\n",
    "    train_resourceset=set(train_listOfResource)\n",
    "    valid_resourceset=set(valid_listOfResource)\n",
    "    all_resource=list(valid_resourceset.union(test_resourceset.union(train_resourceset)))\n",
    "    all_resource.sort()\n",
    "    listOfresourcesInt = list(range(0, len(all_resource)))\n",
    "    resourceMapping=dict(zip(all_resource,listOfresourcesInt))\n",
    "\n",
    "    test_listOfEvents=df_test[\"Activity\"].unique().tolist()\n",
    "    train_listOfEvents=df_train[\"Activity\"].unique().tolist()\n",
    "    valid_listOfEvents=df_valid[\"Activity\"].unique().tolist()\n",
    "    test_eventset=set(test_listOfEvents)\n",
    "    train_eventset=set(train_listOfEvents)\n",
    "    valid_eventset=set(valid_listOfEvents)\n",
    "    all_events=list(valid_eventset.union(test_eventset.union(train_eventset)))\n",
    "    all_events.sort()\n",
    "    listOfeventsInt = list(range(0, len(all_events)))\n",
    "    mapping = dict(zip(all_events, listOfeventsInt))\n",
    "    df_test.Activity = [mapping[item] for item in df_test.Activity]\n",
    "    # 将活动名称和id对应\n",
    "    df_train.Activity = [mapping[item] for item in df_train.Activity]\n",
    "    # 将活动名称和id对应\n",
    "    df_valid.Activity = [mapping[item] for item in df_valid.Activity]\n",
    "\n",
    "    df_test.Resource = [resourceMapping[item] for item in df_test.Resource]\n",
    "    df_train.Resource = [resourceMapping[item] for item in df_train.Resource]\n",
    "    df_valid.Resource = [resourceMapping[item] for item in df_valid.Resource]\n",
    "\n",
    "    act = df_train.groupby('CaseID', sort=False).agg({'Activity': lambda x: list(x)})\n",
    "    res = df_train.groupby('CaseID', sort=False).agg({'Resource': lambda x: list(x)})\n",
    "    temp = df_train.groupby('CaseID', sort=False).agg({'Timestamp': lambda x: list(x)})\n",
    "\n",
    "    act_valid = df_valid.groupby('CaseID', sort=False).agg({'Activity': lambda x: list(x)})\n",
    "    res_valid = df_valid.groupby('CaseID', sort=False).agg({'Resource': lambda x: list(x)})\n",
    "    temp_valid = df_valid.groupby('CaseID', sort=False).agg({'Timestamp': lambda x: list(x)})\n",
    "\n",
    "    act_test = df_test.groupby('CaseID', sort=False).agg({'Activity': lambda x: list(x)})\n",
    "    res_test = df_test.groupby('CaseID', sort=False).agg({'Resource': lambda x: list(x)})\n",
    "    temp_test = df_test.groupby('CaseID', sort=False).agg({'Timestamp': lambda x: list(x)})\n",
    "\n",
    "    act,res,temp=act.loc[:,\"Activity\"].values,res.loc[:,\"Resource\"].values,temp.loc[:,\"Timestamp\"].values\n",
    "    act_valid,res_valid,temp_valid=act_valid.loc[:,\"Activity\"].values,res_valid.loc[:,\"Resource\"].values,temp_valid.loc[:,\"Timestamp\"].values\n",
    "    act_test,res_test,temp_test=act_test.loc[:,\"Activity\"].values,res_test.loc[:,\"Resource\"].values,temp_test.loc[:,\"Timestamp\"].values\n",
    "\n",
    "    train_res_cluster_log=[[res_cluster_label[resource] for resource in res_trace] for res_trace in res]\n",
    "    valid_res_cluster_log=[[res_cluster_label[resource] for resource in res_trace] for res_trace in res_valid]\n",
    "    test_res_cluster_log=[[res_cluster_label[resource] for resource in res_trace] for res_trace in res_test]\n",
    "\n",
    "    \n",
    "    for act_trace in act:\n",
    "        act_trace.append(len(all_events))\n",
    "    for res_trace in res:\n",
    "        res_trace.append(len(all_resource))\n",
    "    for time_trace in temp:\n",
    "        time_trace.append(time_trace[-1])\n",
    "\n",
    "    for act_trace in act_valid:\n",
    "        act_trace.append(len(all_events))\n",
    "    for res_trace in res_valid:\n",
    "        res_trace.append(len(all_resource))\n",
    "    for time_trace in temp_valid:\n",
    "        time_trace.append(time_trace[-1])\n",
    "\n",
    "    for act_trace in act_test:\n",
    "        act_trace.append(len(all_events))\n",
    "    for res_trace in res_test:\n",
    "        res_trace.append(len(all_resource))\n",
    "    for time_trace in temp_test:\n",
    "        time_trace.append(time_trace[-1])\n",
    "    \n",
    "    def distance(prefix,pattern,mode=1):\n",
    "        distance=0\n",
    "        if mode :\n",
    "            a=1 \n",
    "        else:\n",
    "            # b=2\n",
    "            distance=Levenshtein.distance(prefix,pattern)\n",
    "        return distance\n",
    "\n",
    "    def prefix_match(prefix,pattern_dict,sup_list,alpha):\n",
    "        distances=[]\n",
    "        patterns=[]\n",
    "        prefix=[str(key) for key in prefix]\n",
    "        # prefix=prefix.split(\",\")\n",
    "        for key,values in pattern_dict.items():\n",
    "            key=key.split(\",\")\n",
    "            distances.append(Levenshtein.distance(key,prefix))\n",
    "            patterns.append(key)\n",
    "        \n",
    "        min_dis,max_dis=min(distances),max(distances)\n",
    "        if min_dis==max_dis:\n",
    "            distances=[0]*len(distances)\n",
    "        else:\n",
    "            for i in range(len(distances)):\n",
    "                try:\n",
    "                    distances[i]=1-(distances[i]-min_dis)/(max_dis-min_dis)\n",
    "                except ZeroDivisionError :\n",
    "                    print(\"what???\")             \n",
    "                # distances[i]=1-(distances[i]-min_dis)/(max_dis-min_dis)\n",
    "                #             \n",
    "        matchs=[alpha*dis+(1-alpha)*sup for dis,sup in zip(distances,sup_list)]\n",
    "        matched_pattern=patterns[matchs.index(max(matchs))]\n",
    "        return matched_pattern\n",
    "    \n",
    "    rg=re.compile(\"\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}\")\n",
    "    def time_format(ttime):\n",
    "        ttime=rg.search(ttime).group(0)\n",
    "        try:\n",
    "            date_format_str = '%Y/%m/%d %H:%M:%S.%f'\n",
    "            conversion = time.mktime(time.strptime(ttime, date_format_str))\n",
    "        except:\n",
    "            date_format_str = '%Y-%m-%d %H:%M:%S'\n",
    "            conversion = time.mktime(time.strptime(ttime, date_format_str))\n",
    "        return conversion\n",
    "\n",
    "    maxDay=pickle.load(open(\"datasets_results/\"+namedataset+\"/\"+\"fold\"+str(step)+\"/maxInterval_\"+str(alpha)[:3]+\".pkl\",\"rb\"))\n",
    "\n",
    "    def get_max_pattern_length(pattern):\n",
    "        pattern_key_list=[key.split(\",\") for key in  pattern]\n",
    "        pattern_key_length=[len(key) for key in  pattern_key_list]\n",
    "        pattern_key_length.sort()\n",
    "        return pattern_key_length[-1]\n",
    "\n",
    "    act_pattern_length=get_max_pattern_length(pattern_act_dict)\n",
    "    act_res_pattern_length=get_max_pattern_length(pattern_act_res_dict)\n",
    "    act_time_pattern_length=get_max_pattern_length(pattern_act_time_dict)\n",
    "\n",
    "    def prefix_gen(act,res,res_cluster_log,temp):\n",
    "        day_seconds=60*60*24*3\n",
    "        max_day=maxDay\n",
    "        max_last,max_difference=0,0\n",
    "        min_last=1e6\n",
    "        min_remain,max_remain=1e6,0\n",
    "        data=[]\n",
    "        act_pattern_list=[]\n",
    "        act_res_pattern_list=[]\n",
    "        act_time_pattern_list=[]\n",
    "        label=[]\n",
    "        for act_trace,res_trace,res_cluster_trace,time_trace in zip(act,res,res_cluster_log,temp):\n",
    "            for i in range(1,len(act_trace)):\n",
    "                prefix_act=act_trace[:i]\n",
    "                prefix_res=res_trace[:i]\n",
    "                prefix_res_cluster=res_cluster_trace[:i]\n",
    "                # 获取时间向量\n",
    "                temp_time=[time_format(ttime) for ttime in time_trace[:i]]\n",
    "                # 前缀的持续时间轨迹\n",
    "                prefix_last=[time-temp_time[0] for time in temp_time]\n",
    "                # 前缀的间隔时间轨迹\n",
    "                prefix_difference=[ temp_time[m]-temp_time[m-1] if m-1>=0 else 0 for m,time in enumerate (temp_time)]\n",
    "                max_last=max(max_last,max(prefix_last))\n",
    "                min_last=min(min_last,min(prefix_last))\n",
    "                max_difference=max(max_difference,max(prefix_difference))\n",
    "                prefix_difference_interval=[math.floor(different/day_seconds) if (different/day_seconds)<maxDay else maxDay  for different in prefix_difference]\n",
    "                act_time_prefix=[str(activity)+\";\"+str(difference) for activity,difference in zip(prefix_act,prefix_difference_interval)]\n",
    "                act_res_prefix=[str(activity)+\";\"+str(resource) for activity,resource in zip(prefix_act,prefix_res_cluster)]\n",
    "                # pattern=prefix_match(prefix_act,pattern_act_dict,0.5)\n",
    "                act_pattern=prefix_match(prefix_act,pattern_act_dict,pattern_act_sup_list,0.7)\n",
    "                act_res_pattern=prefix_match(act_res_prefix,pattern_act_res_dict,pattern_act_res_sup_list,0.7)\n",
    "                act_time_pattern=prefix_match(act_time_prefix,pattern_act_time_dict,pattern_act_time_sup_list,0.7)\n",
    "\n",
    "                act_label=act_trace[i]\n",
    "                res_label=res_trace[i]\n",
    "                # 获取真实的下一时间间隔值\n",
    "                real_time=(time_format(time_trace[i]))-(time_format(time_trace[i-1])) if i-1>=0 else 0\n",
    "                time_label=(real_time//day_seconds) if (real_time//day_seconds)<maxDay else maxDay \n",
    "                # 获取真实的剩余时间值\n",
    "                real_remain=(time_format(time_trace[-1]))-(time_format(time_trace[i]))\n",
    "                remain_label=(real_remain//day_seconds) if (real_remain//day_seconds)<maxDay else maxDay\n",
    "                min_remain=min(min_remain,real_remain)\n",
    "                max_remain=max(max_remain,real_remain)\n",
    "                # 处理前缀数据，将其转为mean_trace长度\n",
    "                if i>=mean_trace:\n",
    "                    prefix_act=prefix_act[-mean_trace:]\n",
    "                    prefix_res=prefix_res[-mean_trace:]\n",
    "                    prefix_last=prefix_last[-mean_trace:]\n",
    "                    prefix_difference=prefix_difference[-mean_trace:]\n",
    "                else:\n",
    "                    prefix_act=[len(all_events)+1 for j in range(mean_trace-i)]+prefix_act\n",
    "                    prefix_res=[len(all_resource)+1 for j in range(mean_trace-i)]+prefix_res\n",
    "                    prefix_last=[0 for j in range(mean_trace-i)]+prefix_last\n",
    "                    prefix_difference=[0 for j in range(mean_trace-i)]+prefix_difference\n",
    "                \n",
    "\n",
    "                # 将数据加入列表中\n",
    "                # 组合data\n",
    "                prefix_act=np.array(prefix_act).reshape(mean_trace,1)\n",
    "                prefix_res=np.array(prefix_res).reshape(mean_trace,1)\n",
    "                prefix_last=np.array(prefix_last).reshape(mean_trace,1)\n",
    "                prefix_difference=np.array(prefix_difference).reshape(mean_trace,1)\n",
    "                tdata=np.concatenate((prefix_act,prefix_res,prefix_last,prefix_difference),axis=1) #tdata.shape [mean_trae,4]\n",
    "                data.append(tdata)\n",
    "\n",
    "                act_pattern=[int(key) for key in act_pattern]\n",
    "                act_res_pattern=[[int(key.split(\";\")[0]),int(key.split(\";\")[1])] for key in act_res_pattern]\n",
    "                act_time_pattern=[[int(key.split(\";\")[0]),int(key.split(\";\")[1])] for key in act_time_pattern]\n",
    "\n",
    "                if len(act_pattern)<act_pattern_length:\n",
    "                    act_pattern=[len(all_events)+1 for j in range(act_pattern_length-len(act_pattern))] + act_pattern\n",
    "                if len(act_res_pattern)<act_res_pattern_length:\n",
    "                    act_res_pattern=[[len(all_events)+1,res_cluster_num] for j in range(act_res_pattern_length-len(act_res_pattern))] + act_res_pattern\n",
    "                if len(act_time_pattern)<act_time_pattern_length:\n",
    "                    act_time_pattern=[[len(all_events)+1,maxDay+1] for j in range(act_time_pattern_length-len(act_time_pattern))] + act_time_pattern\n",
    "                \n",
    "                act_pattern_list.append(act_pattern)\n",
    "                act_res_pattern_list.append(act_res_pattern)\n",
    "                act_time_pattern_list.append(act_time_pattern)\n",
    "                # 组合label\n",
    "                tlabel=np.array([act_label,res_label,time_label,remain_label,real_time,real_remain])\n",
    "                label.append(tlabel)\n",
    "        \n",
    "        return data,label,act_pattern_list,act_res_pattern_list,act_time_pattern_list,max_last,max_difference,min_last,min_remain,max_remain\n",
    "\n",
    "    train_data,train_label,train_act_pattern_list,train_act_res_pattern_list,train_act_time_pattern_list,train_max_last,train_max_dif,train_min_last,train_min_remain,train_max_remain=prefix_gen(act,res,train_res_cluster_log,temp)\n",
    "    valid_data,valid_label,valid_act_pattern_list,valid_act_res_pattern_list,valid_act_time_pattern_list,valid_max_last,valid_max_dif,valid_min_last,valid_min_remain,valid_max_remain=prefix_gen(act_valid,res_valid,valid_res_cluster_log,temp_valid)\n",
    "    test_data,test_label,test_act_pattern_list,test_act_res_pattern_list,test_act_time_pattern_list,test_max_last,test_max_dif,test_min_last,test_min_remain,test_max_remain=prefix_gen(act_test,res_test,test_res_cluster_log,temp_test)\n",
    "\n",
    "    # if not os.path.exists(\"datasets_results/\"+namedataset+\"/\"+fold+\"/\")\n",
    "    pickle.dump([train_max_remain,valid_max_remain,test_max_remain],open(\"datasets_results/\"+namedataset+\"/\"+fold+\"/time_normlize.pkl\",\"wb\"))\n",
    "    real_max_dif=max([train_max_dif,valid_max_dif,test_max_dif])\n",
    "    real_max_remain=max([train_max_remain,valid_max_remain,test_max_remain])\n",
    "    real_max_last=max([train_max_last,valid_max_last,test_max_last])\n",
    "    for data in train_data:\n",
    "        for row in data:\n",
    "            row[2]=(row[2])/(real_max_last)\n",
    "            row[3]=row[3]/real_max_dif\n",
    "\n",
    "    for data in train_label:\n",
    "        data[4]=data[4]/real_max_dif\n",
    "        # data[5]=(data[5]-min_remain)/(max_remain-min_remain)\n",
    "        data[5]=(data[5])/(real_max_remain)\n",
    "\n",
    "    for data in valid_data:\n",
    "        for row in data:\n",
    "            row[2]=(row[2])/(real_max_last)\n",
    "            row[3]=row[3]/real_max_dif\n",
    "\n",
    "    for data in valid_label:\n",
    "        data[4]=data[4]/real_max_dif\n",
    "        # data[5]=(data[5]-min_remain)/(max_remain-min_remain)\n",
    "        data[5]=(data[5])/(real_max_remain)\n",
    "    \n",
    "    for data in test_data:\n",
    "        for row in data:\n",
    "            row[2]=(row[2])/(real_max_last)\n",
    "            row[3]=row[3]/real_max_dif\n",
    "\n",
    "    for data in test_label:\n",
    "        data[4]=data[4]/real_max_dif\n",
    "        # data[5]=(data[5]-min_remain)/(max_remain-min_remain)\n",
    "        data[5]=(data[5])/(real_max_remain)\n",
    "\n",
    "    if not os.path.exists(\"datasets_results/\"+namedataset+\"/\"+fold+\"/\"+\"trace_length_\"+str(mean_trace)):\n",
    "        os.mkdir(\"datasets_results/\"+namedataset+\"/\"+fold+\"/\"+\"trace_length_\"+str(mean_trace))\n",
    "    \n",
    "    pickle.dump(train_data,open(\"datasets_results/\"+namedataset+\"/\"+fold+\"/\"+\"trace_length_\"+str(mean_trace)+\"/\"+\"train_data_\"+str(alpha)+\".pkl\",\"wb\"))\n",
    "    pickle.dump(train_label,open(\"datasets_results/\"+namedataset+\"/\"+fold+\"/\"+\"trace_length_\"+str(mean_trace)+\"/\"+\"train_label_\"+str(alpha)+\".pkl\",\"wb\"))\n",
    "    pickle.dump(train_act_pattern_list,open(\"datasets_results/\"+namedataset+\"/\"+fold+\"/\"+\"trace_length_\"+str(mean_trace)+\"/\"+\"train_act_pattern_list_\"+str(alpha)+\".pkl\",\"wb\"))\n",
    "    pickle.dump(train_act_res_pattern_list,open(\"datasets_results/\"+namedataset+\"/\"+fold+\"/\"+\"trace_length_\"+str(mean_trace)+\"/\"+\"train_act_res_pattern_list_\"+str(alpha)+\".pkl\",\"wb\"))\n",
    "    pickle.dump(train_act_time_pattern_list,open(\"datasets_results/\"+namedataset+\"/\"+fold+\"/\"+\"trace_length_\"+str(mean_trace)+\"/\"+\"train_act_time_pattern_list_\"+str(alpha)+\".pkl\",\"wb\"))\n",
    "\n",
    "    pickle.dump(test_data,open(\"datasets_results/\"+namedataset+\"/\"+fold+\"/\"+\"trace_length_\"+str(mean_trace)+\"/\"+\"test_data_\"+str(alpha)+\".pkl\",\"wb\"))\n",
    "    pickle.dump(test_label,open(\"datasets_results/\"+namedataset+\"/\"+fold+\"/\"+\"trace_length_\"+str(mean_trace)+\"/\"+\"test_label_\"+str(alpha)+\".pkl\",\"wb\"))\n",
    "    pickle.dump(test_act_pattern_list,open(\"datasets_results/\"+namedataset+\"/\"+fold+\"/\"+\"trace_length_\"+str(mean_trace)+\"/\"+\"test_act_pattern_list_\"+str(alpha)+\".pkl\",\"wb\"))\n",
    "    pickle.dump(test_act_res_pattern_list,open(\"datasets_results/\"+namedataset+\"/\"+fold+\"/\"+\"trace_length_\"+str(mean_trace)+\"/\"+\"test_act_res_pattern_list_\"+str(alpha)+\".pkl\",\"wb\"))\n",
    "    pickle.dump(test_act_time_pattern_list,open(\"datasets_results/\"+namedataset+\"/\"+fold+\"/\"+\"trace_length_\"+str(mean_trace)+\"/\"+\"test_act_time_pattern_list_\"+str(alpha)+\".pkl\",\"wb\"))\n",
    "\n",
    "    pickle.dump(valid_data,open(\"datasets_results/\"+namedataset+\"/\"+fold+\"/\"+\"trace_length_\"+str(mean_trace)+\"/\"+\"valid_data_\"+str(alpha)+\".pkl\",\"wb\"))\n",
    "    pickle.dump(valid_label,open(\"datasets_results/\"+namedataset+\"/\"+fold+\"/\"+\"trace_length_\"+str(mean_trace)+\"/\"+\"valid_label_\"+str(alpha)+\".pkl\",\"wb\"))\n",
    "    pickle.dump(valid_act_pattern_list,open(\"datasets_results/\"+namedataset+\"/\"+fold+\"/\"+\"trace_length_\"+str(mean_trace)+\"/\"+\"valid_act_pattern_list_\"+str(alpha)+\".pkl\",\"wb\"))\n",
    "    pickle.dump(valid_act_res_pattern_list,open(\"datasets_results/\"+namedataset+\"/\"+fold+\"/\"+\"trace_length_\"+str(mean_trace)+\"/\"+\"valid_act_res_pattern_list_\"+str(alpha)+\".pkl\",\"wb\"))\n",
    "    pickle.dump(valid_act_time_pattern_list,open(\"datasets_results/\"+namedataset+\"/\"+fold+\"/\"+\"trace_length_\"+str(mean_trace)+\"/\"+\"valid_act_time_pattern_list_\"+str(alpha)+\".pkl\",\"wb\"))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2e67272651055d1832734db27dbe5c78ae7e6195044c32c87375667fa374755b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
